AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals. It enables you to define data processing workflows called pipelines, which automate the movement and transformation of data, making it easier to manage and monitor complex data workflows. Data Pipeline supports a variety of tasks such as data import/export, data transformation, and scheduled data processing activities, helping organizations streamline data movement and processing across their AWS infrastructure.

Documentation: [Data Pipeline Reference](https://aws.amazon.com/what-is/data-pipeline/)
___
### Overview
#### Components
- Pipeline Definition
- Managed Compute
- Task Runners
- Data Nodes
#### Popular Use Cases
- Processing data in [[EMR]] using Hadoop Streaming
- Importing/exporting [[DynamoDB]] data
- Copying CSV files between [[S3]] buckets
- Exporting [[RDS]] data to [[S3]]
- Copy data to [[Redshift]]

___